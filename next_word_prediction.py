# -*- coding: utf-8 -*-
"""Next_Word_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19X2JZqX48KElBZHkalDGSBfuq-AmGfmc
"""

#!python -m spacy download en_core_web_md

#import spacy

#embedding_file=spacy.load('en_core_web_md')

#embedding_file('want').vector.shape

from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import word_tokenize,sent_tokenize
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

#!pip install nlp

#from nlp import load_dataset
from nltk.tokenize import word_tokenize,sent_tokenize

word_tokenize("Today is a great day!")

import numpy as np
import tensorflow as tf

import nltk
import math
import json
import tensorflow_hub as hub
import spacy

nltk.download('punkt')
nltk.download('gutenberg')

from tensorflow.keras.layers import SimpleRNN,GRU,LSTM,Dense,Embedding,InputLayer,Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import CategoricalCrossentropy
from tensorflow.keras.losses import CategoricalCrossentropy,categorical_crossentropy
from tensorflow.keras import Model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from nltk.corpus import gutenberg

"""data=open("/content/drive/MyDrive/PhD/Data Sources/WhatsApp/corpus.txt")
data=data.read()"""

data=open("0001a.txt")
data=data.read()

"""# New section"""

data

corpus_list=[]
corpus=sent_tokenize(data)
for sent in corpus:
  corpus_list.append([sent])

corpus

len(corpus_list)

vectorizer = TfidfVectorizer()

X = vectorizer.fit_transform(corpus)

X.shape

vocab=vectorizer.get_feature_names_out()

vocab[100]

len(vocab)

embeddings_array=X.toarray().T

embeddings_array[100]

pca=PCA(n_components=10)

dense_embeddings=pca.fit_transform(embeddings_array)

embeddings_dict={}

for i in range(len(vocab)):
  embeddings_dict[vocab[i]]=dense_embeddings[i].tolist()

seq_len=10
tokenizer=Tokenizer()
tokenizer.fit_on_texts([data])
x=tokenizer.texts_to_sequences([data])
#x

x[0]

len(x[0])

#tokenizer.get_config()

vocab_dict=json.loads(tokenizer.get_config().get('word_index'))

vocab_dict

vals=list(vocab_dict.values())
keys=list(vocab_dict.keys())
#keys[vals.index(1)]

vocab_size=len(keys)

vocab_size

embedding_array=np.zeros((vocab_size+1,300))
embedding_array.shape

"""for word,i in tokenizer.word_index.items():
  if word in embeddings_dict:
    embedding_vector=embeddings_dict[word]
  #if embedding_vector is not None:
    embedding_array[i]=embedding_vector"""

for word,i in tokenizer.word_index.items():
  embedding_vector=embedding_file(word).vector
  if embedding_vector is not None:
    embedding_array[i]=embedding_vector

#embedding_array[7306]

len(x[0])

x[0][0:15]

inner_list=[]
X={}
corpus_length=len(x[0])-seq_len
k=0

for i in range(0,corpus_length,seq_len):
  inner_list.clear()
  for j in range(i,i+seq_len):
    inner_list.append(x[0][j])
  X[k]=inner_list[:]
  k=k+1

#list(X.values())
x_train=[]
y_train=[]
for it in list(X.values()):
  x_train.append(it[0:-1])
  y_train.append(it[-1])

x_train[3]

len(x_train)

X=np.array(x_train)
#input_seq.shape
y=np.array(y_train).reshape(len(y_train),1)
#target=tf.reshape(tf.constant(y_train),[len(y_train),1])

y.shape

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)

X_train.shape

y_train

input_seq,test_seq,y_true,y_hat=tf.constant(X_train),tf.constant(X_test),tf.constant(y_train),tf.constant(y_test)

input_seq

y_true

tf.reduce_max(input_seq)+1

input=Input(shape=(seq_len-1,))
x=Embedding(input_dim=vocab_size+1,input_length=seq_len-1,output_dim=20,trainable=True)(input)
x=LSTM(256,return_sequences=True)(x)
x=LSTM(128,return_sequences=True)(x)
x=LSTM(64)(x)
x=Dense(32)(x)
output=Dense(vocab_size,activation="softmax")(x)
model=Model(input,output)

model.summary()

model.compile(optimizer=Adam(0.01),loss=tf.keras.losses.sparse_categorical_crossentropy,metrics=['accuracy'])

model.fit(input_seq,y_true,epochs=10)

model.evaluate(test_seq,y_hat)

input_text=["she was such a good person"]
#input_text=["he was such a good person and honestly speaking he did make good use of his"]
#input_text=["can you please"]
#input_text=["marriage is such a beautiful"]
toks=tokenizer.texts_to_sequences(input_text)
ins=pad_sequences(toks,maxlen=seq_len-1)
input_tensor=tf.constant(ins)
input_tensor

pred=model.predict(input_tensor)
keys[vals.index(np.argmax(pred))]

input_text[0]=input_text[0]+' '+keys[vals.index(np.argmax(pred))]
input_text[0]

for i in range(10):
  toks=tokenizer.texts_to_sequences(input_text)
  ins=pad_sequences(toks,maxlen=seq_len-1)
  input_tensor=tf.constant(ins)
  pred=model.predict(input_tensor)
  input_text[0]=input_text[0]+' '+keys[vals.index(np.argmax(pred))]

print(input_text)

sort_pred=np.argsort(pred[0])
pred_words=np.flip(sort_pred[-20:])
for index in pred_words:
  print(keys[vals.index(index)])